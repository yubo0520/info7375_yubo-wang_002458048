{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b7414af",
   "metadata": {},
   "source": [
    "step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893156d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_and_clean_data(filepath='E_Commerce_Dataset.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"1.Data loaded. Total samples: {len(df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return None\n",
    "\n",
    "    # Using median for filling\n",
    "    df['Tenure'] = df['Tenure'].fillna(df['Tenure'].median())\n",
    "    df['DaySinceLastOrder'] = df['DaySinceLastOrder'].fillna(df['DaySinceLastOrder'].median())\n",
    "    df['CouponUsed'] = df['CouponUsed'].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8a8f6",
   "metadata": {},
   "source": [
    "step 2: MDP Components Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e09af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mdp_components(df):\n",
    "    # A. State Space Definition \n",
    "    # Dimension 1: User Tenure -> New, Mid, Loyal\n",
    "    df['Tenure_Bin'] = pd.qcut(df['Tenure'], 3, labels=[\"New\", \"Mid\", \"Loyal\"]) \n",
    "\n",
    "    # Dimension 2: Recency -> Active, Neutral, Inactive\n",
    "    df['Recency_Bin'] = pd.cut(df['DaySinceLastOrder'], bins=[-1, 2, 7, 1000], labels=[\"Active\", \"Neutral\", \"Inactive\"])\n",
    "    \n",
    "    # Combine dimensions \n",
    "    df['State'] = df['Tenure_Bin'].astype(str) + \"_\" + df['Recency_Bin'].astype(str)\n",
    "    states = sorted(df['State'].unique().tolist())\n",
    "    state_to_idx = {s: i for i, s in enumerate(states)}\n",
    "    num_states = len(states)\n",
    "    \n",
    "    print(\"\\n2.State Space:\")\n",
    "    print(states)\n",
    "\n",
    "    # B. Action Space Definition\n",
    "    # Action 0: Do Nothing\n",
    "    # Action 1: Send Coupon\n",
    "    df['Action'] = (df['CouponUsed'] > 0).astype(int)\n",
    "    num_actions = 2\n",
    "    COUPON_COST = 20 # AssumCost of sending a coupon is $20\n",
    "    \n",
    "    print(\"\\n3.Action Space:\")\n",
    "    print(f\"0: Do Nothing (Cost 0), 1: Send Coupon (Cost {COUPON_COST})\")\n",
    "\n",
    "    # C. Transition Probability & Reward\n",
    "    # The index -1 represents the Churn state\n",
    "    P = np.zeros((num_states, num_actions, num_states + 1))\n",
    "    R = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    REWARD_RETAIN = 100  # Profit from retaining a user\n",
    "    REWARD_CHURN = -100  # Loss from losing a user\n",
    "\n",
    "    for s_idx, s_name in enumerate(states):\n",
    "        for a in [0, 1]:\n",
    "            subset = df[(df['State'] == s_name) & (df['Action'] == a)]\n",
    "            \n",
    "            if len(subset) > 0:\n",
    "                # Calculate churn probability\n",
    "                churn_prob = subset['Churn'].mean()\n",
    "                \n",
    "                # Transition Logic:\n",
    "                P[s_idx, a, -1] = churn_prob\n",
    "                P[s_idx, a, s_idx] = 1.0 - churn_prob\n",
    "                \n",
    "                # Immediate Reward Calculation (E[R|s,a]) \n",
    "                expected_reward = (1 - churn_prob) * REWARD_RETAIN + churn_prob * REWARD_CHURN\n",
    "                \n",
    "                if a == 1: \n",
    "                    expected_reward -= COUPON_COST\n",
    "                \n",
    "                R[s_idx, a] = expected_reward\n",
    "            else:\n",
    "                # Default to a \"pessimistic\" assumption: 100% Churn if no data exists\n",
    "                P[s_idx, a, -1] = 1.0 \n",
    "                R[s_idx, a] = REWARD_CHURN - (COUPON_COST if a == 1 else 0)\n",
    "\n",
    "    return states, P, R, state_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5846b7b",
   "metadata": {},
   "source": [
    "step 3: Policy Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8554ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(states, P, R, gamma=0.9, theta=1e-4):\n",
    "    \"\"\"\n",
    "    Implements the Policy Iteration algorithm required by the assignment.\n",
    "    Includes Policy Evaluation and Policy Improvement steps.\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    \n",
    "    # Initialize Policy: Default to 'Do Nothing' (Action 0) for all states\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    V = np.zeros(num_states + 1) \n",
    "    \n",
    "    is_policy_stable = False\n",
    "    iteration_cnt = 0\n",
    "    \n",
    "    print(\"\\n4.Policy Iteration:\")\n",
    "    \n",
    "    while not is_policy_stable:\n",
    "        iteration_cnt += 1\n",
    "        \n",
    "        # Policy Evaluation \n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(num_states):\n",
    "                v = V[s]\n",
    "                a = policy[s] \n",
    "                \n",
    "                # Bellman Expectation Equation\n",
    "                V[s] = R[s, a] + gamma * (P[s, a, s] * V[s] + P[s, a, -1] * V[-1])\n",
    "                \n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            \n",
    "            if delta < theta:\n",
    "                break \n",
    "        \n",
    "        # Policy Improvement\n",
    "        is_policy_stable = True\n",
    "        \n",
    "        for s in range(num_states):\n",
    "            old_action = policy[s]\n",
    "            \n",
    "            # Find action that maximizes Q-value\n",
    "            best_action = old_action\n",
    "            best_q_value = -float('inf')\n",
    "            \n",
    "            for a in [0, 1]:\n",
    "                # Q(s,a) = R(s,a) + gamma * sum( P(s'|s,a) * V(s') )\n",
    "                q_val = R[s, a] + gamma * (P[s, a, s] * V[s] + P[s, a, -1] * V[-1])\n",
    "                if q_val > best_q_value:\n",
    "                    best_q_value = q_val\n",
    "                    best_action = a\n",
    "            \n",
    "            policy[s] = best_action\n",
    "            \n",
    "            if old_action != best_action:\n",
    "                is_policy_stable = False\n",
    "                \n",
    "        print(f\" -> Iteration {iteration_cnt}: Is policy stable? {is_policy_stable}\")\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868943f5",
   "metadata": {},
   "source": [
    "step 4: Main Execution and Result Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d655b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Data loaded. Total samples: 5630\n",
      "\n",
      "2.State Space:\n",
      "['Loyal_Active', 'Loyal_Inactive', 'Loyal_Neutral', 'Mid_Active', 'Mid_Inactive', 'Mid_Neutral', 'New_Active', 'New_Inactive', 'New_Neutral']\n",
      "\n",
      "3.Action Space:\n",
      "0: Do Nothing (Cost 0), 1: Send Coupon (Cost 20)\n",
      "\n",
      "4.Policy Iteration:\n",
      " -> Iteration 1: Is policy stable? False\n",
      " -> Iteration 2: Is policy stable? True\n",
      "\n",
      "5.FINAL RESULTS:\n",
      "       User State Optimal Policy  Value (LTV)\n",
      "0    Loyal_Active     Do Nothing       727.72\n",
      "1  Loyal_Inactive     Do Nothing       782.89\n",
      "2   Loyal_Neutral     Do Nothing       820.65\n",
      "3      Mid_Active     Do Nothing       258.87\n",
      "4    Mid_Inactive    Send Coupon       565.95\n",
      "5     Mid_Neutral    Send Coupon       419.91\n",
      "6      New_Active     Do Nothing        56.46\n",
      "7    New_Inactive     Do Nothing       161.02\n",
      "8     New_Neutral     Do Nothing       187.07\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = load_and_clean_data()\n",
    "    \n",
    "    if df is not None:\n",
    "        states, P, R, state_to_idx = build_mdp_components(df)\n",
    "        \n",
    "        final_policy, final_V = policy_iteration(states, P, R)\n",
    "        \n",
    "        # Results\n",
    "        results = []\n",
    "        for i, s_name in enumerate(states):\n",
    "            action_str = \"Send Coupon\" if final_policy[i] == 1 else \"Do Nothing\"\n",
    "            results.append({\n",
    "                \"User State\": s_name,\n",
    "                \"Optimal Policy\": action_str,\n",
    "                \"Value (LTV)\": round(final_V[i], 2)\n",
    "            })\n",
    "            \n",
    "        result_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(\"\\n5.FINAL RESULTS:\")\n",
    "        print(result_df)\n",
    "        \n",
    "        result_df.to_csv('mdp_policy_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
